<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta name="author" content="Caleb Owusu-Yianoma">
    <meta name="description" content="Data Structures and Algorithms Manual is a collection of 
                                      articles explaining a variety of core data structures and 
                                      algorithms, with code implementations in Java. Computational 
                                      complexity is used to analyse the efficiency of algorithms 
                                      and operations on data structures.">

    <link href="https://fonts.googleapis.com/css2?family=Commissioner&family=Roboto:wght@300&display=swap" rel="stylesheet">    
    <link href="styles/style.css" rel="stylesheet">

    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="manifest" href="manifest.json">
    
    <title>Computational complexity | Data Structures and Algorithms Manual</title>    
  </head>
  <body class="container">
    <nav class="wide">      
      <ol>
        <li><a href="index.html" class="home-page-link">DATA STRUCTURES AND ALGORITHMS</a></li>
        <li><a href="introduction.html" class="nav-menu-section-item">Introduction</a></li>
        <li><a href="computational-complexity.html" class="nav-menu-section-item">Computational complexity</a></li>
        <li class="nav-menu-header-item">
          Data structures
          <ol>
            <li><a href="data-structures/data-structures-overview.html" class="nav-menu-section-item">Data structures overview</a></li>
            <li><a href="data-structures/array.html" class="nav-menu-section-item">Array</a></li>
            <li><a href="data-structures/dynamic-array.html" class="nav-menu-section-item">Dynamic array</a></li>        
            <li><a href="data-structures/linked-list.html" class="nav-menu-section-item">Linked list</a></li>        
            <li><a href="data-structures/stack.html" class="nav-menu-section-item">Stack</a></li>        
            <li><a href="data-structures/queue.html" class="nav-menu-section-item">Queue</a></li>        
          </ol>
        </li>        
        <li class="nav-menu-header-item">
          Algorithms
          <ol>
            <li><a href="algorithms/algorithms-overview.html" class="nav-menu-section-item">Algorithms overview</a></li>        
            <li><a href="algorithms/binary-search.html" class="nav-menu-section-item">Binary search</a></li> 
            <li><a href="algorithms/dfs.html" class="nav-menu-section-item">Depth-first search</a></li> 
            <li><a href="algorithms/bfs.html" class="nav-menu-section-item">Breadth-first search</a></li> 
            <li><a href="algorithms/dijkstras-shortest-path-algorithm.html" class="nav-menu-section-item">Dijkstra's SSSP algorithm</a></li> 
          </ol>
        </li>
      </ol>
    </nav>

    <main>
      <nav class="narrow">
        <a href="index.html" class="home-page-link">DATA STRUCTURES AND ALGORITHMS</a>
      </nav>
      <nav class="narrow">
        <a href="introduction.html" class="left-nav-link">PREVIOUS</a>
        <a href="data-structures/data-structures-overview.html" class="right-nav-link">NEXT</a>
      </nav>      
      <h1>Computational complexity</h1>
      <p>                
        Before we dive into data structures and algorithms, it's worth discussing computational complexity. 
        As the name suggests, it's a method and a whole field of theoretical computer science that's used 
        to analyse the complexity of computations of all kinds.                 
      </p>
      
      <h2>Motivation</h2>
      <p>
        Suppose you have a computational problem (a problem that a computer might be able to solve) 
        and you need to devise an algorithm to solve it.         
        Depending on the context, you might need a really fast algorithm. 
        For example, if you're developing an algorithm to place trades in a high-frequency trading platform,
        speed is critical.  
      </p>
      <p>                
        Computational complexity is usually used to analyse how quickly algorithms or operations 
        on data structures run. 
        It's also used to analyse how much computer memory certain operations or data structures might require.
        The former type of analysis is usually called "time complexity" and the latter is usually called 
        "space complexity".                
      </p>
      <p>
        Computational complexity is more general than just time and space, though. 
        If you're interested in other applications, check out 
        <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">computational complexity theory</a>.
      </p>
      
      <h2>Time complexity</h2>
      <p>
        Consider an algorithm that takes in some input and, regardless of what the input is, 
        always executes the same 10 steps and then outputs a result. 
        (If you're not familiar with algorithms, think of them as funnels which take in some input, 
        execute some steps and then output a result. 
        We'll cover them some more in the 
        <a href="algorithms/algorithms-overview.html">algorithms overview section</a>.)                
      </p>
      <p>
        No matter how large the input to the algorithm gets, the time the algorithm takes to run will always be the same.
        We could say that the running time of the algorithm is constant with respect to the size of the input. 
        In computational complexity, this is often referred to as a constant-time algorithm. 
      </p>
      <p>        
        Now suppose you have an algorithm that takes in a list of 
        <math>
          <mrow>
            <mi>n</mi>
          </mrow>
        </math> 
        numbers and prints out each of the numbers, one at a time. 
        If we assume that printing a single number takes a constant amount of time, 
        then the running time of this algorithm
        will grow linearly in proportion to the size of the input list. If the input list has size 5, the algorithm will take 5 steps to run.
        If the input list has size 500, the algorithm will take 500 steps to run. 
        Such an algorithm is often referred to as a linear-time algorithm.
      </p>
      <p>
        Say we modify the previous algorithm slightly. This time, it takes in a sorted list of 
        <math>          
          <mi>n</mi>          
        </math>
        consecutive integers starting with 1 - for example, <code>[1, 2, 3, 4, 5]</code>.
        Instead of simply printing out each number, it prints out the sum of all the positive integers
        less than or equal to the number. 
      </p>
      <p>
        So, in this case, the algorithm would print: <code>1, 3, 6, 10, 15</code>.
      </p>
      <p>
        What's the running time of this algorithm? If we assume that each sum operation takes constant time, 
        then we just need to calculate the number of sum operations 
        and add that to the number of print operations. There'll be <math><mi>n</mi></math> print operations and, 
        for a given number <math><mi>i</mi></math> in the list, there'll be 
        <math>
          <mrow>
            <mi>i</mi>
            <mo>-</mo>
            <mn>1</mn>
          </mrow>
        </math> sum operations.
      </p>
      <p>
        Since this is a list of consecutive integers starting at 1, the total number of sum operations will be 
        <math>
          <mrow>
            <mn>0</mn>
            <mo>+</mo>
            <mn>1</mn>
            <mo>+</mo>
            <mn>2</mn>
            <mo>+</mo>
            &#x2026;
            <mo>+</mo>
            <mo>(</mo>
            <mi>n</mi>
            <mo>-</mo>
            <mn>1</mn>
            <mo>)</mo>
          </mrow>
        </math>. 
        This is an arithmetic series which sums to 
        <math>
          <mrow>            
            <mi>n</mi>
            <mo>(</mo>
            <mi>n</mi>
            <mo>-</mo>
            <mn>1</mn>
            <mo>)</mo>
            <mo>/</mo>
            <mn>2</mn>
          </mrow>
        </math>. See this 
        <a href="https://www.mathsisfun.com/algebra/triangular-numbers.html">triangular number sequence article</a> 
        for a visual derivation.
      </p>      
      <p>
        So the total number of steps the algorithm 
        takes for an input list of size 
        <math>
          <mi>n</mi>
        </math> 
        is quadratic with respect to <math><mi>n</mi></math>.        
        Such an algorithm is referred to as a quadratic-time algorithm. 
      </p>
      <p>
        There are a whole host of other running times - for example, 
        logarithmic, cubic, exponential and even algorithms which run in roughly 
        <math>
          <mrow>
            <mi>n</mi>
            <mo>!</mo>
          </mrow>
        </math>
        (n factorial)
        steps, where <math><mi>n</mi></math> is the input size. 
        In order to reason about all of these more effectively, people who discuss computational complexity 
        often use a particular notation called "big O". Let's look at that next.
      </p>
      
      <h2>Big O notation</h2>
      <p>
        When I was writing this, I suddenly wondered why big O is called "big O". The letter "O" is used 
        because this notation is used to reason about the "order" of various mathematical functions. 
        More specifically, when studying computational complexity, we're often most interested in the growth rates of  
        functions of some input size <math><mi>n</mi></math>, say, and the growth rate is also known as the 
        order of the function. 
      </p>
      <p>
        For example, the time complexity of the constant-time algorithm we discussed earlier 
        is written as 
        <math>
          <mrow>
            <mi>O</mi>
            <mo>(</mo>
            <mn>1</mn>
            <mo>)</mo>
          </mrow>
        </math>        
        in big O notation. This is pronounced as "big O of 1" or "order 1" 
        and it means that, given an input size of <math><mi>n</mi></math>, the algorithm in question takes a constant number of steps 
        in the worst case, with respect to <math><mi>n</mi></math>.        
      </p>
      <p>
        "In the worst case?" Yes, an algorithm might take a different number of steps depending on the type of input it receives, 
        even if the input size is fixed. For example, suppose you have an algorithm that takes in an input list of 
        <math><mi>n</mi></math> numbers 
        and then sorts the numbers in ascending order and outputs the result. If the input list is already sorted in ascending order, 
        then the algorithm doesn't need to do anything. However, if the input list is sorted in descending order, the algorithm will 
        have to do a lot more work to sort the list. This is an example of a worst-case input for a given input size.
      </p>
      <p>
        The linear-time algorithm from above is denoted as 
        <math>
          <mrow>
            <mi>O</mi>
            <mo>(</mo>
            <mi>n</mi>
            <mo>)</mo>
          </mrow>
        </math>        
        and pronounced as "big O of n" or "order n". 
        An interesting point to note is that, if the algorithm takes 
        <math>
          <mrow>
            <mn>2</mn>
            <mi>n</mi>
          </mrow>
        </math>        
        steps instead of <math><mi>n</mi></math>, the worst-case running time of the algorithm 
        is still denoted as 
        <math>
          <mrow>
            <mi>O</mi>
            <mo>(</mo>
            <mi>n</mi>
            <mo>)</mo>
          </mrow>
        </math>,        
        not 
        <math>
          <mrow>
            <mi>O</mi>
            <mo>(</mo>
            <mn>2</mn>
            <mi>n</mi>
            <mo>)</mo>
          </mrow>
        </math>.        
        Why?
        The reason we're able to drop the constant is that, when we analyse computational complexity,
        we're most interested in the rate of growth of the function in question as the input size <math><mi>n</mi></math> gets very large.
        As <math><mi>n</mi></math> gets very large, the constant <math><mn>2</mn></math> in 
        <math>
          <mrow>
            <mn>2</mn>
            <mi>n</mi>
          </mrow>
        </math>        
        affects the rate of growth much less than the variable <math><mi>n</mi></math>.
      </p>
      <p>
        Similarly, if we have an algorithm that takes 
        <math>
          <mrow>
            <mn>2</mn>
            <msup>
              <mi>n</mi>
              <mn>2</mn>            
            </msup>     
            <mo>+</mo>       
            <mi>n</mi>
          </mrow>
        </math>        
        steps, the rate of growth for very large input sizes is 
        most affected by the highest order term, 
        <math>
          <mrow>
            <msup>
              <mi>n</mi>
              <mn>2</mn>
            </msup>
          </mrow>
        </math>.         
        Therefore, in big O notation, we drop the lower order term and the 
        constant and say that the running time is 
        <math>
          <mrow>
            <mi>O</mi>
            <mo>(</mo>
            <msup>
              <mi>n</mi>
              <mn>2</mn>
            </msup>            
            <mo>)</mo>
          </mrow>
        </math>.        
      </p>
            
      <p>
        It's also helpful to note that the usage of big O notation in the technology industry often differs from its usage
        in academia. In academia, big O notation is used to describe an upper bound on the computational complexity of 
        some computation. Therefore, the linear-time algorithm we saw earlier could be described as 
        <math>
          <mrow>
            <mi>O</mi>
            <mo>(</mo>
            <msup>
              <mi>n</mi>
              <mn>2</mn>
            </msup>            
            <mo>)</mo>
          </mrow>
        </math>,        
        even though 
        that's not a tight bound. However, in industry, big O notation is usually used to describe a tight bound on the 
        computational complexity. Therefore, that algorithm would be described as 
        <math>
          <mrow>
            <mi>O</mi>
            <mo>(</mo>
            <mi>n</mi>                                        
            <mo>)</mo>
          </mrow>
        </math>.
        In the worst case, it runs in 
        linear time and in the best case it runs in linear time. 
      </p>
      <p>
        If you're like me and love to dig deeper into the details, see 
        <cite><a href="https://mitpress.mit.edu/books/introduction-algorithms-third-edition">Introduction to Algorithms, Third Edition</a></cite>, 
        often referred to as "CLRS", for a more formal treatment of big O notation.
      </p>

      <h2>Space complexity</h2>
      <p>
        Sometimes, it's important to analyse how much memory a given operation or algorithm will require. 
        This is where space complexity comes into play. As with time complexity, we usually use big O notation to 
        reason about space complexity.
      </p>
      <p>
        A formal treatment of this topic would require
        us to specify the computational model and device in great detail. Most of the time, however, 
        it's generally assumed that the algorithms or operations are implemented as computer programs 
        and that each step is executed one after the other. 
      </p>

      <p>
        This doesn't account for more complex models, like algorithms that have steps that can be run in parallel, 
        but for the purposes of this manual, it's sufficient. Again, for more formality, check out 
        <cite><a href="https://mitpress.mit.edu/books/introduction-algorithms-third-edition">Introduction to Algorithms, Third Edition</a></cite>. 
      </p>

      <p>
        As a brief example of space complexity, the linear-time algorithm we saw earlier 
        which simply prints out each of the numbers in the input list has a space complexity which is also linear, 
        if we assume that the input is an <a href="data-structures/array.html">array</a>. Why? 
        This is because memory must be allocated for each of the <math><mi>n</mi></math> elements in the input list. 
      </p>
      
      <h2>A helpful resource</h2>
      <p>
        A final note: I came across this <a href="https://www.bigocheatsheet.com/">big O cheat sheet</a>
        a while ago. It's a really helpful summary of various time and space complexities, 
        especially if you're prepping for upcoming technical interviews. 
      </p>
      <p>
        Next up are <a href="data-structures/data-structures-overview.html">data structures</a>.
      </p>
      <nav>
        <a href="introduction.html" class="left-nav-link">PREVIOUS</a>
        <a href="data-structures/data-structures-overview.html" class="right-nav-link">NEXT</a>
      </nav>
    </main>

    <footer>      
      <hr class="footer-top-rule">
      Crafted by Caleb Owusu-Yianoma - &copy; 2020
    </footer>

    <!-- A light MathML polyfill recommended by MDN -->
    <!-- See https://developer.mozilla.org/en-US/docs/Web/MathML/Authoring -->
    <!-- <script src="https://fred-wang.github.io/mathml.css/mspace.js"></script> -->
    <script src="mspace.js"></script>
  </body>
</html>